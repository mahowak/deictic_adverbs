{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3e1f54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "from run_ib import RunIB\n",
    "from enumerate_lexicons import get_random_lexicon\n",
    "import enumerate_lexicons as el\n",
    "import random\n",
    "from ib import mi, information_plane\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stirling import stirling\n",
    "import enumerate_lexicons\n",
    "pd.options.display.max_rows = 250\n",
    "import scipy\n",
    "from helper_functions import *\n",
    "import run_ib_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c58368b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: _ib (this includes custom initialization)\n",
    "DEFAULT_NUM_ITER=10\n",
    "PRECISION = 1e-16\n",
    "def _ib(p_x, p_y_x, Z, gamma, init, num_iter=DEFAULT_NUM_ITER, temperature = 1):\n",
    "    \"\"\" Find encoder q(Z|X) to minimize J = I[X:Z] - gamma * I[Y:Z].\n",
    "    \n",
    "    Input:\n",
    "    p_x : Distribution on X, of shape X.\n",
    "    p_y_x : Conditional distribution on Y given X, of shape X x Y.\n",
    "    gamma : A non-negative scalar value.\n",
    "    Z : Support size of Z.\n",
    "\n",
    "    Output: \n",
    "    Conditional distribution on Z given X, of shape X x Z.\n",
    "\n",
    "    \"\"\"\n",
    "    # Support size of X\n",
    "    X = p_x.shape[-1]\n",
    "\n",
    "    # Support size of Y\n",
    "    Y = p_y_x.shape[-1]\n",
    "\n",
    "    # Randomly initialize the conditional distribution q(z|x)\n",
    "    q_z_x = init #scipy.special.softmax(np.random.randn(X, Z), -1) # shape X x Z\n",
    "    p_y_x = p_y_x[:, None, :] # shape X x 1 x Y\n",
    "    p_x = p_x[:, None] # shape X x 1\n",
    "\n",
    "    # Blahut-Arimoto iteration to find the minimizing q(z|x)\n",
    "    for _ in range(num_iter):\n",
    "        q_xz = p_x * q_z_x # Joint distribution q(x,z), shape X x Z\n",
    "        q_z = q_xz.sum(axis=0, keepdims=True) # Marginal distribution q(z), shape 1 x Z\n",
    "        q_y_z = ((q_xz / q_z)[:, :, None] * p_y_x).sum(axis=0, keepdims=True) # Conditional decoder distribution q(y|z), shape 1 x Z x Y\n",
    "        d = ( \n",
    "            scipy.special.xlogy(p_y_x, p_y_x)\n",
    "            - scipy.special.xlogy(p_y_x, q_y_z) # negative KL divergence -D[p(y|x) || q(y|z)]\n",
    "        ).sum(axis=-1) # expected distortion over Y; shape X x Z\n",
    "        q_z_x = scipy.special.softmax((np.log(q_z) - gamma*d)/temperature, axis=-1) # Conditional encoder distribution q(z|x) = 1/Z q(z) e^{-gamma*d}\n",
    "\n",
    "    return q_z_x\n",
    "\n",
    "num_dists = 3\n",
    "pgs_dists = [0,0.789,-1.315]\n",
    "num_words = 9\n",
    "num_meanings = num_dists * 3\n",
    "\n",
    "# function to make the non-deterministic frontier\n",
    "def make_curve(mu, logsp=np.logspace(2, 0, num=1500), pgs=pgs_dists):\n",
    "    init = np.identity(num_words)\n",
    "\n",
    "    qW_M = []\n",
    "    informativity = []\n",
    "    complexity = []\n",
    "\n",
    "    for gamma in logsp:\n",
    "        x = RunIB(mu, gamma, num_dists, pgs_dists)\n",
    "        p_m = x.prior\n",
    "        p_u_m = x.prob_u_given_m\n",
    "        q_w_m = _ib(p_m, p_u_m, num_words, gamma, init, num_iter = 20)\n",
    "        informativity_temp, complexity_temp = information_plane(p_m, p_u_m, q_w_m)\n",
    "\n",
    "        qW_M.append(q_w_m)\n",
    "        informativity.append(informativity_temp)\n",
    "        complexity.append(complexity_temp)\n",
    "        init = q_w_m\n",
    "        \n",
    "    curve = pd.DataFrame(data = {'gamma': logsp,\n",
    "                    'informativity' : informativity,\n",
    "                    'complexity' : complexity,\n",
    "                    'J' : complexity - logsp*informativity})\n",
    "    return curve, qW_M\n",
    "\n",
    "# function to find the objective function\n",
    "def get_objective(p_m, p_u_m, q_w_m, gamma):\n",
    "    informativity, complexity = information_plane(p_m, p_u_m, q_w_m)\n",
    "    return complexity - gamma * informativity\n",
    "\n",
    "# function to find gamma minimizing the objective function\n",
    "def find_gamma_index(p_m, p_u_m, q_w_m, curve):\n",
    "    objs = np.array([get_objective(p_m, p_u_m, q_w_m, gamma) for gamma in logsp])\n",
    "    objs_2 = curve[\"J\"].values\n",
    "    diff = objs - objs_2\n",
    "    return(diff.argmin())\n",
    "\n",
    "# function to calculate efficiency loss (Zaslavsky et al., 2018)\n",
    "def find_epsilon(p_m, p_u_m, q_w_m, curve):\n",
    "    objs = np.array([get_objective(p_m, p_u_m, q_w_m, gamma) for gamma in logsp])\n",
    "    objs_2 = curve[\"J\"].values\n",
    "    diff = objs - objs_2\n",
    "    return(diff.min()/logsp[diff.argmin()])\n",
    "\n",
    "# codes from Zaslavsky et al. (2018)\n",
    "def xlogx(v):\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        return np.where(v > PRECISION, v * np.log2(v), 0)\n",
    "    \n",
    "def H(p, axis=None):\n",
    "    \"\"\" Entropy \"\"\"\n",
    "    return -xlogx(p).sum(axis=axis)\n",
    "\n",
    "def MI(pXY):\n",
    "    \"\"\" mutual information, I(X;Y) \"\"\"\n",
    "    return H(pXY.sum(axis=0)) + H(pXY.sum(axis=1)) - H(pXY)\n",
    "\n",
    "# function to calculate gNID (Zaslavsky et al., 2018)\n",
    "def gNID(pW_X, pV_X, pX):\n",
    "    if len(pX.shape) == 1:\n",
    "        pX = pX[:, None]\n",
    "    elif pX.shape[0] == 1 and pX.shape[1] > 1:\n",
    "        pX = pX.T\n",
    "    pXW = pW_X * pX\n",
    "    pWV = pXW.T.dot(pV_X)\n",
    "    pWW = pXW.T.dot(pW_X)\n",
    "    pVV = (pV_X * pX).T.dot(pV_X)\n",
    "    score = 1 - MI(pWV) / (np.max([MI(pWW), MI(pVV)]))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e703afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part generates the non-deterministic optimal frontier, with gamma ranging from 1 to 1000\n",
    "logsp = np.logspace(3,0,num = 2000)\n",
    "mu = 0.3\n",
    "curve, qW_M = make_curve(mu, logsp, pgs_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33ccaf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part generates all possible lexicons\n",
    "x = RunIB(mu,2,num_dists, pgs_dists)\n",
    "num_meanings = 3 * num_dists\n",
    "lexicon_size_range = range(2, num_meanings + 1)\n",
    "sim_lex_dict = {lexicon_size: [lexicon for lexicon in enumerate_lexicons.enumerate_possible_lexicons(num_meanings, lexicon_size)] for \n",
    "        lexicon_size in lexicon_size_range}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36544621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame containing real lexicons\n",
    "lexicons = x.get_real_langs(num_meanings)\n",
    "\n",
    "df = pd.DataFrame([{dm: l[1].argmax(axis=1)[dm_num]\n",
    "                        for dm_num, dm in enumerate(x.deictic_index)} for l in lexicons])\n",
    "information_plane_list = [information_plane(x.prior, x.prob_u_given_m, l[1]) for l in lexicons]\n",
    "df[\"I[U;W]\"] = [l[0] for l in information_plane_list]\n",
    "df[\"I[M;W]\"] = [l[1] for l in information_plane_list]\n",
    "df[\"gamma_fit\"] = [logsp[find_gamma_index(x.prior, x.prob_u_given_m, l[1], curve)] for l in lexicons]\n",
    "df[\"epsilon\"] = [find_epsilon(x.prior, x.prob_u_given_m, l[1], curve) for l in lexicons]\n",
    "df[\"gNID\"] = [gNID(l[1], qW_M[find_gamma_index(x.prior, x.prob_u_given_m, l[1], curve)], x.prior) for l in lexicons]\n",
    "df[\"Language\"] = [l[0] for l in lexicons]\n",
    "df[\"Area\"] = [l[2] for l in lexicons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae786d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frame containing simulated lexicons\n",
    "lexicons_sim = []\n",
    "for lexicon_size in range(2, num_meanings+1):\n",
    "    all_lex = sim_lex_dict[lexicon_size]\n",
    "    lexicons_sim += [(\"simulated\", l[1], \"simulated\") for l in all_lex]\n",
    "\n",
    "df_sim = pd.DataFrame([{dm: l[1].argmax(axis=1)[dm_num]\n",
    "                        for dm_num, dm in enumerate(x.deictic_index)} for l in lexicons_sim])\n",
    "\n",
    "information_plane_list_sim = [information_plane(x.prior, x.prob_u_given_m, l[1]) for l in lexicons_sim]\n",
    "df_sim[\"I[U;W]\"] = [l[0] for l in information_plane_list_sim]\n",
    "df_sim[\"I[M;W]\"] = [l[1] for l in information_plane_list_sim]\n",
    "df_sim[\"gamma_fit\"] = [logsp[find_gamma_index(x.prior, x.prob_u_given_m, l[1], curve)] for l in lexicons_sim]\n",
    "df_sim[\"epsilon\"] = [find_epsilon(x.prior, x.prob_u_given_m, l[1], curve) for l in lexicons_sim]\n",
    "df_sim[\"gNID\"] = [gNID(l[1], qW_M[find_gamma_index(x.prior, x.prob_u_given_m, l[1], curve)], x.prior) for l in lexicons_sim]\n",
    "df_sim[\"Language\"] = [l[0] for l in lexicons_sim]\n",
    "df_sim[\"Area\"] = [l[2] for l in lexicons_sim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe0a0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = systematicity(df)\n",
    "df_sim = systematicity(df_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f892ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save things\n",
    "df.to_csv('sheets/real_lexicons_fit_mu_' + str(mu) + '_pgs_' + \"_\".join([str(pgs) for pgs in pgs_dists]) + 'num_dists_' + str(num_dists) + '.csv')\n",
    "df_sim.to_csv('sheets/sim_lexicons_fit_mu_'+ str(mu) + '_pgs_' + \"_\".join([str(pgs) for pgs in pgs_dists]) + 'num_dists_' + str(num_dists) +'.csv')\n",
    "curve.to_csv('sheets/ib_curve_non_deter_mu_' + str(mu) + '_pgs_' + \"_\".join([str(pgs) for pgs in pgs_dists]) + 'num_dists_' + str(num_dists) +'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b98915",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lexicon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d52172657025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunIB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgs_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlexicons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_real_langs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_meanings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrun_ib_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRunIB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgs_dists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mi_for_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lexicon' is not defined"
     ]
    }
   ],
   "source": [
    "mu = 0.3\n",
    "num_dists = 3\n",
    "pgs_dists = [0,0.789,-1.315]\n",
    "num_words = 9\n",
    "num_meanings = num_dists * 3\n",
    "x = RunIB(mu,2,num_dists, pgs_dists)\n",
    "lexicons = x.get_real_langs(num_meanings)\n",
    "temp = [run_ib_new.RunIB(mu, num_dists, pgs_dists).find_everything(l[1]) for l in lexicons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b9a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [tup[0] for tup in temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c25881",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4eaa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_lexicons_ordered = [run_ib_new.RunIB(mu, num_dists, pgs_dists).optimal_lexicons[int(idx)] for idx in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371f3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = run_ib_new.RunIB(mu, num_dists, pgs_dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649bf2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = list(cls.deictic_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6504acbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3439750113173663"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
