{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac3fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from run_ib import RunIB\n",
    "from enumerate_lexicons import get_random_lexicon\n",
    "import enumerate_lexicons as el\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import math\n",
    "import einops\n",
    "from stirling import stirling\n",
    "import enumerate_lexicons\n",
    "pd.options.display.max_rows = 250\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "from scipy.stats import linregress\n",
    "import scipy\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5305d730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mi_torch(p_xy):\n",
    "#     \"\"\" Calculate mutual information of a distribution P(x,y) \n",
    "\n",
    "#     Input: \n",
    "#     p_xy: An X x Y array giving p(x,y)\n",
    "    \n",
    "#     Output:\n",
    "#     The mutual information I[X:Y], a nonnegative scalar,\n",
    "#     \"\"\"\n",
    "#     p_x = p_xy.sum(axis=-1, keepdims=True)\n",
    "#     p_y = p_xy.sum(axis=-2, keepdims=True)\n",
    "#     return torch.xlogy(p_xy, p_xy).sum() - torch.xlogy(p_x, p_x).sum() - torch.xlogy(p_y, p_y).sum()\n",
    "\n",
    "# def information_plane_torch(p_x, p_y_x, p_z_x):\n",
    "#     \"\"\" Given p(x), p(y|x), and p(z|x), calculate I[Y:Z] and I[X:Z] \"\"\"\n",
    "#     p_xz = torch.mul(p_x[:, None] , p_z_x) # Joint p(x,y), shape X x Y    \n",
    "#     p_xyz = torch.mul(torch.mul(p_x[:, None, None] , p_y_x[:, :, None]) , p_z_x[:, None, :]) # Joint p(x,y,z), shape X x Y x Z\n",
    "#     p_yz = p_xyz.sum(axis=0) # Joint p(y,z), shape Y x Z\n",
    "#     return mi_torch(p_yz), mi_torch(p_xz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0afa6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logsp = np.logspace(0,2,num = 1500)\n",
    "# mu = 0.3\n",
    "# num_dists = 3\n",
    "# pgs_dists = [0,0.789,-1.315]\n",
    "# num_words = 3\n",
    "\n",
    "# gamma = 2\n",
    "\n",
    "# num_meanings = num_dists * 3\n",
    "# x = RunIB(mu, gamma, num_dists, pgs_dists)\n",
    "\n",
    "# p_m = torch.tensor(x.prior) # shape: M\n",
    "# p_u_m = torch.tensor(x.prob_u_given_m) # shape: M x U\n",
    "\n",
    "# #initialize encoder q(w|m)\n",
    "# q_w_m_init = torch.rand(num_meanings, num_words, requires_grad = True) # shape: M x W\n",
    "# m = torch.nn.Softmax(dim = -1)\n",
    "\n",
    "\n",
    "# # set up optimizer\n",
    "# opt = torch.optim.AdamW([q_w_m_init], lr = 2)\n",
    "\n",
    "# print(m(q_w_m_init))\n",
    "\n",
    "# for epoch in range(10000):\n",
    "#     q_w_m = m(q_w_m_init) # normalize such that each row sums up to 1\n",
    "#     informativity, complexity = information_plane_torch(p_m, p_u_m, q_w_m)\n",
    "#     loss = complexity - gamma * informativity\n",
    "#     loss.backward()\n",
    "#     opt.step()\n",
    "#     opt.zero_grad()\n",
    "\n",
    "# m(q_w_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a57363f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def information_plane(p_x, p_y_x, p_z_x):\n",
    "#     \"\"\" Given p(x), p(y|x), and p(z|x), calculate I[Y:Z] and I[X:Z] \"\"\"\n",
    "#     p_xz = p_x[:, None] * p_z_x # Joint p(x,y), shape X x Y    \n",
    "#     p_xyz = p_x[:, None, None] * p_y_x[:, :, None] * p_z_x[:, None, :] # Joint p(x,y,z), shape X x Y x Z\n",
    "#     p_yz = p_xyz.sum(axis=0) # Joint p(y,z), shape Y x Z\n",
    "#     return mi(p_yz), mi(p_xz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a8cba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now let's try scipy.optimize.basinhopping\n",
    "\n",
    "# logsp = np.logspace(0,2,num = 1500)\n",
    "# mu = 0.3\n",
    "# num_dists = 3\n",
    "# pgs_dists = [0,0.789,-1.315]\n",
    "# num_words = 6\n",
    "\n",
    "# gamma = logsp[200]\n",
    "\n",
    "# num_meanings = num_dists * 3\n",
    "# x = RunIB(mu, gamma, num_dists, pgs_dists)\n",
    "\n",
    "# p_m = x.prior\n",
    "# p_u_m = x.prob_u_given_m\n",
    "\n",
    "# def ib_score(q_w_m, p_m, p_u_m, gamma):\n",
    "#     if q_w_m.ndim == 1:\n",
    "#         q_w_m = einops.rearrange(q_w_m, '(meaning word) -> meaning word', word = num_words)\n",
    "#     q_w_m = scipy.special.softmax(q_w_m, axis = -1)\n",
    "#     informativity, complexity = information_plane(p_m, p_u_m, q_w_m)\n",
    "#     return complexity - gamma * informativity  \n",
    "\n",
    "# q_w_m = np.random.rand(num_meanings, num_words)\n",
    "# func = lambda x: ib_score(x, p_m, p_u_m, gamma)\n",
    "# minimizer_kwargs = {\"method\": \"BFGS\"}\n",
    "\n",
    "# results = scipy.optimize.basinhopping(func=func, x0=q_w_m, stepsize = 0.5, niter=200, minimizer_kwargs=minimizer_kwargs)\n",
    "\n",
    "# res = scipy.special.softmax(einops.rearrange(results.x, '(meaning word) -> meaning word', word = num_words), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0376770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 3: reversed deterministic annealing\n",
    "DEFAULT_NUM_ITER = 10\n",
    "\n",
    "def ib(p_x, p_y_x, Z, gamma, num_iter=DEFAULT_NUM_ITER, outer_iter=10, temperature = 1):\n",
    "    q_z_xs = [_ib(p_x, p_y_x, Z, gamma, num_iter, temperature) for i in range(outer_iter)]\n",
    "    iplanes = [information_plane(p_x, p_y_x, q) for q in q_z_xs]\n",
    "    J = np.array([i[1] - gamma * i[0] for i in iplanes])\n",
    "    return q_z_xs[J.argmin()]\n",
    "\n",
    "def _ib(p_x, p_y_x, Z, gamma, init, num_iter=DEFAULT_NUM_ITER, temperature = 1):\n",
    "    \"\"\" Find encoder q(Z|X) to minimize J = I[X:Z] - gamma * I[Y:Z].\n",
    "    \n",
    "    Input:\n",
    "    p_x : Distribution on X, of shape X.\n",
    "    p_y_x : Conditional distribution on Y given X, of shape X x Y.\n",
    "    gamma : A non-negative scalar value.\n",
    "    Z : Support size of Z.\n",
    "\n",
    "    Output: \n",
    "    Conditional distribution on Z given X, of shape X x Z.\n",
    "\n",
    "    \"\"\"\n",
    "    # Support size of X\n",
    "    X = p_x.shape[-1]\n",
    "\n",
    "    # Support size of Y\n",
    "    Y = p_y_x.shape[-1]\n",
    "\n",
    "    # Randomly initialize the conditional distribution q(z|x)\n",
    "    q_z_x = init #scipy.special.softmax(np.random.randn(X, Z), -1) # shape X x Z\n",
    "    p_y_x = p_y_x[:, None, :] # shape X x 1 x Y\n",
    "    p_x = p_x[:, None] # shape X x 1\n",
    "\n",
    "    # Blahut-Arimoto iteration to find the minimizing q(z|x)\n",
    "    for _ in range(num_iter):\n",
    "        q_xz = p_x * q_z_x # Joint distribution q(x,z), shape X x Z\n",
    "        q_z = q_xz.sum(axis=0, keepdims=True) # Marginal distribution q(z), shape 1 x Z\n",
    "        q_y_z = ((q_xz / q_z)[:, :, None] * p_y_x).sum(axis=0, keepdims=True) # Conditional decoder distribution q(y|z), shape 1 x Z x Y\n",
    "        d = ( \n",
    "            scipy.special.xlogy(p_y_x, p_y_x)\n",
    "            - scipy.special.xlogy(p_y_x, q_y_z) # negative KL divergence -D[p(y|x) || q(y|z)]\n",
    "        ).sum(axis=-1) # expected distortion over Y; shape X x Z\n",
    "        q_z_x = scipy.special.softmax((np.log(q_z) - gamma*d)/temperature, axis=-1) # Conditional encoder distribution q(z|x) = 1/Z q(z) e^{-gamma*d}\n",
    "\n",
    "    return q_z_x\n",
    "\n",
    "def mi(p_xy):\n",
    "    \"\"\" Calculate mutual information of a distribution P(x,y) \n",
    "\n",
    "    Input: \n",
    "    p_xy: An X x Y array giving p(x,y)\n",
    "    \n",
    "    Output:\n",
    "    The mutual information I[X:Y], a nonnegative scalar,\n",
    "    \"\"\"\n",
    "    p_x = p_xy.sum(axis=-1, keepdims=True)\n",
    "    p_y = p_xy.sum(axis=-2, keepdims=True)\n",
    "    return scipy.special.xlogy(p_xy, p_xy).sum() - scipy.special.xlogy(p_x, p_x).sum() - scipy.special.xlogy(p_y, p_y).sum()\n",
    "\n",
    "def information_plane(p_x, p_y_x, p_z_x):\n",
    "    \"\"\" Given p(x), p(y|x), and p(z|x), calculate I[Y:Z] and I[X:Z] \"\"\"\n",
    "    p_xz = p_x[:, None] * p_z_x # Joint p(x,y), shape X x Y    \n",
    "    p_xyz = p_x[:, None, None] * p_y_x[:, :, None] * p_z_x[:, None, :] # Joint p(x,y,z), shape X x Y x Z\n",
    "    p_yz = p_xyz.sum(axis=0) # Joint p(y,z), shape Y x Z\n",
    "    return mi(p_yz), mi(p_xz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logsp = np.logspace(0,2,num = 1500)\n",
    "mu = 0.3\n",
    "num_dists = 3\n",
    "pgs_dists = [0,0.789,-1.315]\n",
    "num_words = 6\n",
    "num_meanings = num_dists * 3\n",
    "x = RunIB(mu, gamma, num_dists, pgs_dists)\n",
    "\n",
    "p_m = x.prior\n",
    "p_u_m = x.prob_u_given_m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
